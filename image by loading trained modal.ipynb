{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce4e39ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pandas\n",
    "import numpy as np\n",
    "import graphviz\n",
    "import pydot\n",
    "import tensorflow as tf\n",
    "from tqdm.auto import tqdm\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical , plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
    "\n",
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model(\"F:\\image captioning\\kaggle\\working\\modelcompile.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "909096e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"F:\\image captioning\\kaggle\\input\\Flickr8k\"\n",
    "WORKING_DIR = 'F:\\image captioning\\kaggle\\working'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d544afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(BASE_DIR, 'captions.txt'), 'r') as f:\n",
    "    next(f)\n",
    "    captions_doc = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad364458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0652b5d4930f4ea9ada042022d1568fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create mapping of image to captions\n",
    "mapping = {}\n",
    "# process lines\n",
    "for line in tqdm(captions_doc.split('\\n')):\n",
    "    # split the line by comma(,)\n",
    "    tokens = line.split(',')\n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    image_id, caption = tokens[0], tokens[1:]\n",
    "    # remove extension from image ID\n",
    "    image_id = image_id.split('.')[0]\n",
    "    # convert caption list to string\n",
    "    caption = \" \".join(caption)\n",
    "    # create list if needed\n",
    "    if image_id not in mapping:\n",
    "        mapping[image_id] = []\n",
    "    # store the caption\n",
    "    mapping[image_id].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1106bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(mapping):\n",
    "    for key, captions in mapping.items():\n",
    "        for i in range(len(captions)):\n",
    "            # take one caption at a time\n",
    "            caption = captions[i]\n",
    "            # preprocessing steps\n",
    "            # convert to lowercase\n",
    "            caption = caption.lower()\n",
    "            # delete digits, special chars, etc., \n",
    "            caption = caption.replace('[^A-Za-z]', '')\n",
    "            # delete additional spaces\n",
    "            caption = caption.replace('\\s+', ' ')\n",
    "            # add start and end tags to the caption\n",
    "            caption = 'startseq ' + \" \".join([word for word in         caption.split() if len(word)>1]) + ' endseq'\n",
    "            captions[i] = caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e662446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A child in a pink dress is climbing up a set of stairs in an entry way .',\n",
       " 'A girl going into a wooden building .',\n",
       " 'A little girl climbing into a wooden playhouse .',\n",
       " 'A little girl climbing the stairs to her playhouse .',\n",
       " 'A little girl in a pink dress going into a wooden cabin .']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before preprocess of text\n",
    "mapping['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3877faa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean(mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d634235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['startseq child in pink dress is climbing up set of stairs in an entry way endseq',\n",
       " 'startseq girl going into wooden building endseq',\n",
       " 'startseq little girl climbing into wooden playhouse endseq',\n",
       " 'startseq little girl climbing the stairs to her playhouse endseq',\n",
       " 'startseq little girl in pink dress going into wooden cabin endseq']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af261f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions = []\n",
    "for key in mapping:\n",
    "    for caption in mapping[key]:\n",
    "        all_captions.append(caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2784705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2d7cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get maximum length of the caption available\n",
    "max_length = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb8c658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def idx_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d313725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "675e44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate caption for an image\n",
    "def predict_caption(model, image, tokenizer, max_length):\n",
    "    # add start tag for generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the max length of sequence\n",
    "    for i in range(max_length):\n",
    "        # encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad the sequence\n",
    "        sequence = pad_sequences([sequence], max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([image, sequence], verbose=0)\n",
    "        # get index with high probability\n",
    "        yhat = np.argmax(yhat)\n",
    "        # convert index to word\n",
    "        word = idx_to_word(yhat, tokenizer)\n",
    "        # stop if word not found\n",
    "        if word is None:\n",
    "            break\n",
    "        # append word as input for generating next word\n",
    "        in_text += \" \" + word\n",
    "        # stop if we reach end tag\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "      \n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a218e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = VGG16()\n",
    "# restructure the model\n",
    "vgg_model = Model(inputs=vgg_model.inputs, outputs=vgg_model.layers[-2].output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8ae8eae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\SATYANKAR\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 2416, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\SATYANKAR\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 2401, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\SATYANKAR\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 2389, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\SATYANKAR\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 2357, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\SATYANKAR\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\SATYANKAR\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 1 of layer \"model_2\" is incompatible with the layer: expected shape=(None, 75), found shape=(None, 35)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m feature \u001b[38;5;241m=\u001b[39m vgg_model\u001b[38;5;241m.\u001b[39mpredict(image, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# predict from the trained model\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m predict_caption(model, feature, tokenizer, max_length)\n",
      "Cell \u001b[1;32mIn[28], line 12\u001b[0m, in \u001b[0;36mpredict_caption\u001b[1;34m(model, image, tokenizer, max_length)\u001b[0m\n\u001b[0;32m     10\u001b[0m sequence \u001b[38;5;241m=\u001b[39m pad_sequences([sequence], max_length)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# predict next word\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m yhat \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict([image, sequence], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# get index with high probability\u001b[39;00m\n\u001b[0;32m     14\u001b[0m yhat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(yhat)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\SATYAN~1\\AppData\\Local\\Temp\\__autograph_generated_file1c5l0zu5.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\SATYANKAR\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 2416, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\SATYANKAR\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 2401, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\SATYANKAR\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 2389, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\SATYANKAR\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 2357, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\SATYANKAR\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\SATYANKAR\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 1 of layer \"model_2\" is incompatible with the layer: expected shape=(None, 75), found shape=(None, 35)\n"
     ]
    }
   ],
   "source": [
    "image_path = 'F:\\image captioning\\kaggle\\input\\Flickr8k\\Images\\Formalphoto.jpeg'\n",
    "# load image\n",
    "image = load_img(image_path, target_size=(224, 224))\n",
    "# convert image pixels to numpy array\n",
    "image = img_to_array(image)\n",
    "# reshape data for model\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "# preprocess image for vgg\n",
    "image = preprocess_input(image)\n",
    "# extract features\n",
    "feature = vgg_model.predict(image, verbose=0)\n",
    "# predict from the trained model\n",
    "predict_caption(model, feature, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c10b3f",
   "metadata": {},
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have train_loss, val_loss, train_accuracy, val_accuracy lists\n",
    "# Visualize Epoch vs. Loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_loss) + 1), train_loss, label='Training Loss')\n",
    "plt.plot(range(1, len(val_loss) + 1), val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Epoch vs. Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Visualize Epoch vs. Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_accuracy) + 1), train_accuracy, label='Training Accuracy')\n",
    "plt.plot(range(1, len(val_accuracy) + 1), val_accuracy, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Epoch vs. Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Assuming you have actual and predicted values for METEOR calculation\n",
    "# Calculate METEOR score\n",
    "from nltk.translate import meteor_score\n",
    "\n",
    "meteor_scores = [meteor_score.single_meteor_score(actual[i], predicted[i]) for i in range(len(actual))]\n",
    "average_meteor_score = sum(meteor_scores) / len(meteor_scores)\n",
    "print(\"Average METEOR Score: {:.4f}\".format(average_meteor_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b84bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c29690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
